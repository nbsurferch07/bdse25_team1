{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adb7897-a1bb-444b-979a-18d2f8fd710f",
   "metadata": {},
   "source": [
    "### main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f609055-6a8a-4ea4-9912-ac1be67f30ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page 1 finished\n",
      "page 2 finished\n",
      "page 3 finished\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlretrieve\n",
    "import time\n",
    "import csv\n",
    "import os\n",
    "from os.path import exists\n",
    "import shutil\n",
    "\n",
    "\n",
    "\n",
    "# browser driver setting\n",
    "s = Service(\"./chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "def get_page_info(city_name, page_range):\n",
    "    page_info_list = []\n",
    "    for page in range(1, page_range + 1):\n",
    "        driver.get('https://www.sinyi.com.tw/buy/list/apartment-dalou-huaxia-flat-townhouse-villa-type/' + city_name + '/' + str(page)) \n",
    "        time.sleep(5)\n",
    "        \n",
    "        info = driver.page_source\n",
    "        time.sleep(5)\n",
    "        page_info_list.append(info)\n",
    "        \n",
    "    driver.close()   \n",
    "    return page_info_list\n",
    "\n",
    "def get_item_info(info):\n",
    "    \n",
    "    soup1 = BeautifulSoup(info, 'html.parser')\n",
    "    div_tags = soup1.find_all('div', {'class':'buy-list-item'})\n",
    "\n",
    "    soup2 = BeautifulSoup(info, 'html.parser')\n",
    "    items = soup2.find_all('div', {'class':'LongInfoCard_Type_Left'})\n",
    "\n",
    "    item_info_list = []\n",
    "    for i, item in zip(items, div_tags):\n",
    "        # id \n",
    "        href_info = str(item)\n",
    "        item_id = href_info.split('/buy/house/')[-1].split('/?breadcrumb=list')[0]\n",
    "        # name\n",
    "        item_name = str(i.find_all('div', {'class':'LongInfoCard_Type_Name'})).split('LongInfoCard_Type_Name\">')[-1].split('<div>')[0]\n",
    "        # address\n",
    "        item_address = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[0].split('<span>')[-1]\n",
    "        item_age = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[1]\n",
    "        item_type = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[2].split('</span>')[0]\n",
    "        # info\n",
    "        item_size_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[0].split('建坪 ')[-1]    # 建坪\n",
    "        item_size_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[1].split('主 + 陽')[-1]    # 主+陽\n",
    "        item_room = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[2]\n",
    "        item_floor = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[3].split('</span>')[0]\n",
    "        # specific tags\n",
    "        item_tag_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[0].split('specificTag\">')[-1]\n",
    "        item_tag_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[1]\n",
    "        item_tag_3 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[2].split('</span>')[0]\n",
    "        \n",
    "        item_info_list.append([item_id, item_name, item_address, item_age, item_type, item_size_1, item_size_2, item_room, item_floor, item_tag_1, item_tag_2, item_tag_3])\n",
    "    return (item_info_list)\n",
    "\n",
    "\n",
    "def save_image(item_info_list):\n",
    "    try:\n",
    "        for i in item_info_list:\n",
    "            src = 'https://res.sinyi.com.tw/buy/' + i[0] + '/bigimg/E.JPG'\n",
    "            urlretrieve(src, './images/' + i[0] + '.jpg')\n",
    "            # print('save image: {}.jpg'.format(img_no))\n",
    "            # time.sleep(2)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def write_csv(item_info):\n",
    "    with open('result.csv', 'a', encoding='utf-8-sig') as f:\n",
    "        f.write(\n",
    "            item_info[0] + ',' + \n",
    "            item_info[1] + ',' + \n",
    "            item_info[2] + ',' +\n",
    "            item_info[3] + ',' + \n",
    "            item_info[4] + ',' + \n",
    "            item_info[5] + ',' +\n",
    "            item_info[6] + ',' + \n",
    "            item_info[7] + ',' + \n",
    "            item_info[8] + ',' +\n",
    "            item_info[9] + ',' + \n",
    "            item_info[10] + ',' +\n",
    "            item_info[11] + '\\n')\n",
    "        \n",
    "\n",
    "# set city\n",
    "city_name = 'Taipei-city'\n",
    "# set page range\n",
    "page_range = 3\n",
    "# set image number from 1 start\n",
    "img_no = 0\n",
    "# clear data\n",
    "if exists('./result.csv'):\n",
    "    os.remove('./result.csv')\n",
    "if exists('./images'):\n",
    "    shutil.rmtree('./images')\n",
    "    os.makedirs('images')\n",
    "    \n",
    "\n",
    "\n",
    "count = 0\n",
    "page_info_list = get_page_info(city_name, page_range)\n",
    "\n",
    "for info in page_info_list:\n",
    "    item_info_list = get_item_info(info)\n",
    "    save_image(item_info_list)\n",
    "    \n",
    "    for item_info in item_info_list:\n",
    "        write_csv(item_info)\n",
    "        \n",
    "    count += 1\n",
    "    print('page {} finished'.format(count))\n",
    "    \n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28288a52-8687-4b87-99c3-94f3fe221369",
   "metadata": {},
   "source": [
    "### Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc43e11-f872-4a77-b7ab-cf4849618819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from urllib.request import urlretrieve\n",
    "import time\n",
    "\n",
    "# browser driver setting\n",
    "s = Service(\"./chromedriver.exe\")\n",
    "driver = webdriver.Chrome(service=s)\n",
    "\n",
    "# set city\n",
    "city_name = 'Taipei-city'\n",
    "\n",
    "driver.get('https://www.sinyi.com.tw/buy/list/apartment-dalou-huaxia-flat-townhouse-villa-type/' + city_name) \n",
    "time.sleep(5)\n",
    "\n",
    "\n",
    "info = driver.page_source\n",
    "# info = driver.page_source\n",
    "time.sleep(5)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6068c0-f980-4937-af21-5a3057faf654",
   "metadata": {},
   "source": [
    "### get house ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c9388-5501-437f-8cef-61ed2ee1ff52",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(info, 'html.parser')\n",
    "div_tags = soup.find_all('div', {'class':'buy-list-item'})\n",
    "\n",
    "houseID_list = []\n",
    "for i in div_tags:\n",
    "    href_info = str(i)\n",
    "    house_id = href_info.split('/buy/house/')[-1].split('/?breadcrumb=list')[0]\n",
    "    houseID_list.append(house_id)\n",
    "    # urls =  'https://www.sinyi.com.tw/buy/house/' + house_id\n",
    "    # url_list.append(urls)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef2105f-42a9-421c-b519-8ce570e63a0b",
   "metadata": {},
   "source": [
    "### Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7834d98-86a5-4527-b340-c2fbc609353b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "soup1 = BeautifulSoup(info, 'html.parser')\n",
    "div_tags = soup1.find_all('div', {'class':'buy-list-item'})\n",
    "\n",
    "soup2 = BeautifulSoup(info, 'html.parser')\n",
    "items = soup2.find_all('div', {'class':'LongInfoCard_Type_Left'})\n",
    "\n",
    "    \n",
    "for i, item in zip(items, div_tags):\n",
    "    # id \n",
    "    href_info = str(item)\n",
    "    item_id = href_info.split('/buy/house/')[-1].split('/?breadcrumb=list')[0]\n",
    "    # name\n",
    "    item_name = str(i.find_all('div', {'class':'LongInfoCard_Type_Name'})).split('LongInfoCard_Type_Name\">')[-1].split('<div>')[0]\n",
    "    # address\n",
    "    item_address = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[0].split('<span>')[-1]\n",
    "    item_age = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[1]\n",
    "    item_type = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[2].split('</span>')[0]\n",
    "    # info\n",
    "    item_size_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[0].split('建坪 ')[-1]    # 建坪\n",
    "    item_size_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[1].split('主 + 陽')[-1]    # 主+陽\n",
    "    item_room = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[2]\n",
    "    item_floor = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[3].split('</span>')[0]\n",
    "    # specific tags\n",
    "    item_tag_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[0].split('specificTag\">')[-1]\n",
    "    item_tag_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[1]\n",
    "    item_tag_3 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[2].split('</span>')[0]\n",
    "\n",
    "    print('{},{},{},{},{},{},{},{},{},{},{},{}'.format(\n",
    "        item_id, item_name, item_address, item_age, item_type, item_size_1, item_size_2, item_room, item_floor, item_tag_1, item_tag_2, item_tag_3)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397d0d4a-5ef0-43b0-80bf-10c124990bfa",
   "metadata": {},
   "source": [
    "### write to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdcfb7b8-3eba-4725-bf5b-3d01326c1822",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "soup = BeautifulSoup(info, 'html.parser')\n",
    "div_tags = soup.find_all('div', {'class':'buy-list-item'})\n",
    "items = soup.find_all('div', {'class':'LongInfoCard_Type_Left'})\n",
    "\n",
    "\n",
    "with open('result.csv', 'w') as f:\n",
    "    for i in items:\n",
    "        # name\n",
    "        item_name = str(i.find_all('div', {'class':'LongInfoCard_Type_Name'})).split('LongInfoCard_Type_Name\">')[-1].split('<div>')[0]\n",
    "        # address\n",
    "        item_address = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[0].split('<span>')[-1]\n",
    "        item_age = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[1]\n",
    "        item_type = str(i.find_all('div', {'class':'LongInfoCard_Type_Address'})).split('</span><span>')[2].split('</span>')[0]\n",
    "        # info\n",
    "        item_size_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[0].split('建坪 ')[-1]    # 建坪\n",
    "        item_size_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[1].split('主 + 陽')[-1]    # 主+陽\n",
    "        item_room = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[2]\n",
    "        item_floor = str(i.find_all('div', {'class':'LongInfoCard_Type_HouseInfo'})).split('</span><span>')[3].split('</span>')[0]\n",
    "        # specific tags\n",
    "        item_tag_1 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[0].split('specificTag\">')[-1]\n",
    "        item_tag_2 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[1]\n",
    "        item_tag_3 = str(i.find_all('div', {'class':'LongInfoCard_Type_SpecificTags'})).split('</span><span class=\"specificTag\">')[2].split('</span>')[0]\n",
    "\n",
    "        f.write(\n",
    "            item_name + ',' + \n",
    "            item_address + ',' + \n",
    "            item_age +\n",
    "            item_type + ',' + \n",
    "            item_size_1 + ',' + \n",
    "            item_size_2 +\n",
    "            item_room + ',' + \n",
    "            item_floor + ',' + \n",
    "            item_tag_1 +\n",
    "            item_tag_2 + ',' + \n",
    "            item_tag_3 + ',' +\n",
    "            '\\n')\n",
    "        \n",
    "        print('{},{},{},{},{},{},{},{},{},{},{}'.format(\n",
    "            item_name, item_address, item_age, item_type, item_size_1, item_size_2, item_room, item_floor, item_tag_1, item_tag_2, item_tag_3)\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d331031f-bb67-4de9-a73a-62c3206f9176",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "    \n",
    "soup = BeautifulSoup(info, 'html.parser')\n",
    "div_tags = soup.find_all('div', {'class':'buy-list-item'})\n",
    "\n",
    "count = 0\n",
    "houseID_list = []\n",
    "for i in div_tags:\n",
    "    href_info = str(i)\n",
    "    house_id = href_info.split('/buy/house/')[-1].split('/?breadcrumb=list')[0]\n",
    "    houseID_list.append(house_id)\n",
    "    # urls =  'https://www.sinyi.com.tw/buy/house/' + house_id\n",
    "    # url_list.append(urls)\n",
    "\n",
    "    \n",
    "for i in houseID_list:\n",
    "    src = 'https://res.sinyi.com.tw/buy/' + i + '/bigimg/E.JPG'\n",
    "    img_no += 1\n",
    "    urlretrieve(src, './images/' + str(img_no) + '.jpg')\n",
    "    print('save image: {}.jpg'.format(img_no))\n",
    "    time.sleep(2)\n",
    "\n",
    "\n",
    "def get_houseID(info):\n",
    "    \n",
    "    soup = BeautifulSoup(info, 'html.parser')\n",
    "    div_tags = soup.find_all('div', {'class':'buy-list-item'})\n",
    "\n",
    "    count = 0\n",
    "    houseID_list = []\n",
    "    for i in div_tags:\n",
    "        href_info = str(i)\n",
    "        house_id = href_info.split('/buy/house/')[-1].split('/?breadcrumb=list')[0]\n",
    "        houseID_list.append(house_id)\n",
    "        # urls =  'https://www.sinyi.com.tw/buy/house/' + house_id\n",
    "        # url_list.append(urls)\n",
    "    return houseID_list\n",
    "\n",
    "\n",
    "img_no = 0\n",
    "for i in url_list:\n",
    "    src = 'https://res.sinyi.com.tw/buy/' + i + '/bigimg/E.JPG'\n",
    "    img_no += 1\n",
    "    urlretrieve(src, './images/' + str(img_no) + '.jpg')\n",
    "    print('save image: {} .jpg'.format(img_no))\n",
    "    time.sleep(2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
