{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubiCasa / FloorplantoBlenderlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#downloading weights \n",
    "\n",
    "!gdown 'https://drive.google.com/uc?id=1gRB7ez1e4H7a9Y09lLqRuna0luZO5VRK'\n",
    "\n",
    "#downloading blender\n",
    "\n",
    "blender_url =  \"https://ftp.nluug.nl/pub/graphics/blender/release/Blender2.93/blender-2.93.1-linux-x64.tar.xz\"\n",
    "base_url = os.path.basename(blender_url)\n",
    "!mkdir $blender_version\n",
    "!wget -nc $blender_url\n",
    "!tar -xkf $base_url -C ./$blender_version --strip-components=1\n",
    "!rm blender-2.93.1-linux-x64.tar.xz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add outer folder \n",
    " # Adds higher directory to python modules path.\n",
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "\n",
    "# Other necessary libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input Image\n",
    "Now we need an example image to work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input image: ![input](Images/example4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Contours (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=650x488>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "\n",
    "import cv2 # for image gathering\n",
    "import numpy as np\n",
    "\n",
    "# for visualize\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "img_path = \"Images/example4.png\"\n",
    "\n",
    "# Read floorplan image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# Create blank image\n",
    "height, width, channels = img.shape\n",
    "blank_image = np.zeros((height,width,3), np.uint8)\n",
    "\n",
    "# Grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect outer Contours (simple floor or roof solution), paint them red on blank_image\n",
    "contour, img = detect.detectOuterContours(gray, blank_image, color=(255,0,0))\n",
    "\n",
    "# Display\n",
    "display(Image.fromarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Rooms (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=650x488>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = cv2.imread(img_path)\n",
    "\n",
    "    # grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# create verts (points 3d), points to use in mesh creations\n",
    "verts = []\n",
    "# create faces for each plane, describe order to create mesh points\n",
    "faces = []\n",
    "\n",
    "# Height of waLL\n",
    "height = 0.999\n",
    "\n",
    "# Scale pixel value to 3d pos\n",
    "scale = 100\n",
    "\n",
    "gray = detect.wall_filter(gray)\n",
    "\n",
    "gray = ~gray\n",
    "\n",
    "rooms, colored_rooms = detect.find_rooms(gray.copy())\n",
    "\n",
    "gray_rooms =  cv2.cvtColor(colored_rooms,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "contours, hierarchy = cv2.findContours(gray_rooms,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "area=sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "\n",
    "# get box positions for rooms\n",
    "boxes, gray_rooms= detect.detectPreciseBoxes(gray_rooms, gray_rooms)\n",
    "display(Image.fromarray(colored_rooms))\n",
    "\n",
    " #Create verts\n",
    "room_count = 0\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, height)])\n",
    "    room_count+= 1\n",
    "\n",
    "# create faces\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for pos in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate room area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63786.5\n",
      "20381.5\n",
      "14478.0\n",
      "8778.0\n",
      "5819.5\n"
     ]
    }
   ],
   "source": [
    "# contours, hierarchy = cv2.findContours(gray_rooms,cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)\n",
    "# area=sorted(contours, key=cv2.contourArea, reverse=True)\n",
    "for c in area:\n",
    "    mask = np.zeros(gray.shape, dtype=\"uint8\")  # 依Contours 圖形建立 mask\n",
    "    cv2.drawContours(mask, [c],  -1, 255, -1) #255→白色, -1→塗滿\n",
    "    # show the images\n",
    "    # cv2.imshow(\"Image\", gray_rooms)\n",
    "    cv2.imshow(\"Mask\", mask)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    contour_area = cv2.contourArea(c)\n",
    "    print(contour_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function display in module IPython.core.display_functions:\n",
      "\n",
      "display(*objs, include=None, exclude=None, metadata=None, transient=None, display_id=None, raw=False, clear=False, **kwargs)\n",
      "    Display a Python object in all frontends.\n",
      "    \n",
      "    By default all representations will be computed and sent to the frontends.\n",
      "    Frontends can decide which representation is used and how.\n",
      "    \n",
      "    In terminal IPython this will be similar to using :func:`print`, for use in richer\n",
      "    frontends see Jupyter notebook examples with rich display logic.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *objs : object\n",
      "        The Python objects to display.\n",
      "    raw : bool, optional\n",
      "        Are the objects to be displayed already mimetype-keyed dicts of raw display data,\n",
      "        or Python objects that need to be formatted before display? [default: False]\n",
      "    include : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to include in the\n",
      "        format data dict. If this is set *only* the format types included\n",
      "        in this list will be computed.\n",
      "    exclude : list, tuple or set, optional\n",
      "        A list of format type strings (MIME types) to exclude in the format\n",
      "        data dict. If this is set all format types will be computed,\n",
      "        except for those included in this argument.\n",
      "    metadata : dict, optional\n",
      "        A dictionary of metadata to associate with the output.\n",
      "        mime-type keys in this dictionary will be associated with the individual\n",
      "        representation formats, if they exist.\n",
      "    transient : dict, optional\n",
      "        A dictionary of transient data to associate with the output.\n",
      "        Data in this dict should not be persisted to files (e.g. notebooks).\n",
      "    display_id : str, bool optional\n",
      "        Set an id for the display.\n",
      "        This id can be used for updating this display area later via update_display.\n",
      "        If given as `True`, generate a new `display_id`\n",
      "    clear : bool, optional\n",
      "        Should the output area be cleared before displaying anything? If True,\n",
      "        this will wait for additional output before clearing. [default: False]\n",
      "    **kwargs : additional keyword-args, optional\n",
      "        Additional keyword-arguments are passed through to the display publisher.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    handle: DisplayHandle\n",
      "        Returns a handle on updatable displays for use with :func:`update_display`,\n",
      "        if `display_id` is given. Returns :any:`None` if no `display_id` is given\n",
      "        (default).\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> class Json(object):\n",
      "    ...     def __init__(self, json):\n",
      "    ...         self.json = json\n",
      "    ...     def _repr_pretty_(self, pp, cycle):\n",
      "    ...         import json\n",
      "    ...         pp.text(json.dumps(self.json, indent=2))\n",
      "    ...     def __repr__(self):\n",
      "    ...         return str(self.json)\n",
      "    ...\n",
      "    \n",
      "    >>> d = Json({1:2, 3: {4:5}})\n",
      "    \n",
      "    >>> print(d)\n",
      "    {1: 2, 3: {4: 5}}\n",
      "    \n",
      "    >>> display(d)\n",
      "    {\n",
      "      \"1\": 2,\n",
      "      \"3\": {\n",
      "        \"4\": 5\n",
      "      }\n",
      "    }\n",
      "    \n",
      "    >>> def int_formatter(integer, pp, cycle):\n",
      "    ...     pp.text('I'*integer)\n",
      "    \n",
      "    >>> plain = get_ipython().display_formatter.formatters['text/plain']\n",
      "    >>> plain.for_type(int, int_formatter)\n",
      "    <function _repr_pprint at 0x...>\n",
      "    >>> display(7-5)\n",
      "    II\n",
      "    \n",
      "    >>> del plain.type_printers[int]\n",
      "    >>> display(7-5)\n",
      "    2\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    :func:`update_display`\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    In Python, objects can declare their textual representation using the\n",
      "    `__repr__` method. IPython expands on this idea and allows objects to declare\n",
      "    other, rich representations including:\n",
      "    \n",
      "      - HTML\n",
      "      - JSON\n",
      "      - PNG\n",
      "      - JPEG\n",
      "      - SVG\n",
      "      - LaTeX\n",
      "    \n",
      "    A single object can declare some or all of these representations; all are\n",
      "    handled by IPython's display system.\n",
      "    \n",
      "    The main idea of the first approach is that you have to implement special\n",
      "    display methods when you define your class, one for each representation you\n",
      "    want to use. Here is a list of the names of the special methods and the\n",
      "    values they must return:\n",
      "    \n",
      "      - `_repr_html_`: return raw HTML as a string, or a tuple (see below).\n",
      "      - `_repr_json_`: return a JSONable dict, or a tuple (see below).\n",
      "      - `_repr_jpeg_`: return raw JPEG data, or a tuple (see below).\n",
      "      - `_repr_png_`: return raw PNG data, or a tuple (see below).\n",
      "      - `_repr_svg_`: return raw SVG data as a string, or a tuple (see below).\n",
      "      - `_repr_latex_`: return LaTeX commands in a string surrounded by \"$\",\n",
      "                        or a tuple (see below).\n",
      "      - `_repr_mimebundle_`: return a full mimebundle containing the mapping\n",
      "                             from all mimetypes to data.\n",
      "                             Use this for any mime-type not listed above.\n",
      "    \n",
      "    The above functions may also return the object's metadata alonside the\n",
      "    data.  If the metadata is available, the functions will return a tuple\n",
      "    containing the data and metadata, in that order.  If there is no metadata\n",
      "    available, then the functions will return the data only.\n",
      "    \n",
      "    When you are directly writing your own classes, you can adapt them for\n",
      "    display in IPython by following the above approach. But in practice, you\n",
      "    often need to work with existing classes that you can't easily modify.\n",
      "    \n",
      "    You can refer to the documentation on integrating with the display system in\n",
      "    order to register custom formatters for already existing types\n",
      "    (:ref:`integrating_rich_display`).\n",
      "    \n",
      "    .. versionadded:: 5.4 display available without import\n",
      "    .. versionadded:: 6.1 display available without import\n",
      "    \n",
      "    Since IPython 5.4 and 6.1 :func:`display` is automatically made available to\n",
      "    the user without import. If you are using display in a document that might\n",
      "    be used in a pure python context or with older version of IPython, use the\n",
      "    following import at the top of your file::\n",
      "    \n",
      "        from IPython.display import display\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Walls (Object Filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=650x488>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''\n",
    "    Generate wall data file for floorplan\n",
    "    @Param img_path, path to input file\n",
    "    @Param info, boolean if data should be printed\n",
    "    @Return shape\n",
    "    '''\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "# Read floorplan image\n",
    "img = cv2.imread(img_path)\n",
    "\n",
    "# grayscale image\n",
    "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# create wall image (filter out small objects from image)\n",
    "wall_img = detect.wall_filter(gray)\n",
    "\n",
    "# detect walls\n",
    "boxes, img = detect.detectPreciseBoxes(wall_img)\n",
    "\n",
    "display(Image.fromarray(wall_img))\n",
    "\n",
    "# create verts (points 3d), points to use in mesh creations\n",
    "verts = []\n",
    "# create faces for each plane, describe order to create mesh points\n",
    "faces = []\n",
    "\n",
    "# Height of waLL\n",
    "wall_height = 1\n",
    "\n",
    "# Scale pixel value to 3d pos\n",
    "scale = 100\n",
    "\n",
    "# Convert boxes to verts and faces\n",
    "verts, faces, wall_amount = transform.create_nx4_verts_and_faces(boxes, wall_height, scale)\n",
    "\n",
    "# Create top walls verts\n",
    "verts = []\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, 0)])\n",
    "\n",
    "# create faces\n",
    "faces = []\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for _ in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module utils.FloorplanToBlenderLib.detect in utils.FloorplanToBlenderLib:\n",
      "\n",
      "NAME\n",
      "    utils.FloorplanToBlenderLib.detect\n",
      "\n",
      "FUNCTIONS\n",
      "    detectAndRemovePreciseBoxes(detect_img, output_img=None, color=[255, 255, 255])\n",
      "        Remove contours of detected walls from image\n",
      "        @Param detect_img image to detect from @mandatory\n",
      "        @Param output_img image for output\n",
      "        @Param color to set on output\n",
      "        @Return list of boxes, actual image\n",
      "        @source https://stackoverflow.com/questions/50930033/drawing-lines-and-distance-to-them-on-image-opencv-python\n",
      "    \n",
      "    detectOuterContours(detect_img, output_img=None, color=[255, 255, 255])\n",
      "        Get the outer side of floorplan, used to get ground\n",
      "        @Param detect_img image to detect from @mandatory\n",
      "        @Param output_img image for output\n",
      "        @Param color to set on output\n",
      "        @Return approx, box\n",
      "        @Source https://stackoverflow.com/questions/50930033/drawing-lines-and-distance-to-them-on-image-opencv-python\n",
      "    \n",
      "    detectPreciseBoxes(detect_img, output_img=None, color=[100, 100, 0])\n",
      "        Detect corners with boxes in image with high precision\n",
      "        @Param detect_img image to detect from @mandatory\n",
      "        @Param output_img image for output\n",
      "        @Param color to set on output\n",
      "        @Return corners(list of boxes), output image\n",
      "        @source https://stackoverflow.com/questions/50930033/drawing-lines-and-distance-to-them-on-image-opencv-python\n",
      "    \n",
      "    find_corners_and_draw_lines(img, corners_threshold, room_closing_max_length)\n",
      "        Finds corners and draw lines from them\n",
      "        Help function for finding room\n",
      "        @Param image input image\n",
      "        @Param corners_threshold threshold for corner distance\n",
      "        @Param room_closing_max_length threshold for room max size\n",
      "        @Return output image\n",
      "    \n",
      "    find_details(img, noise_removal_threshold=50, corners_threshold=0.01, room_closing_max_length=130, gap_in_wall_max_threshold=5000, gap_in_wall_min_threshold=10)\n",
      "        !!! Currently not used in IMPLEMENTATION !!!\n",
      "        I have copied and changed this function some...\n",
      "        \n",
      "        origin from\n",
      "        https://stackoverflow.com/questions/54274610/crop-each-of-them-using-opencv-python\n",
      "        \n",
      "        @Param img: grey scale image of rooms, already eroded and doors removed etc.\n",
      "        @Param noise_removal_threshold: Minimal area of blobs to be kept.\n",
      "        @Param corners_threshold: Threshold to allow corners. Higher removes more of the house.\n",
      "        @Param room_closing_max_length: Maximum line length to add to close off open doors.\n",
      "        @Param gap_in_wall_threshold: Minimum number of pixels to identify component as room instead of hole in the wall.\n",
      "        @Return: rooms: list of numpy arrays containing boolean masks for each detected room\n",
      "                 colored_house: A colored version of the input image, where each room has a random color.\n",
      "    \n",
      "    find_rooms(img, noise_removal_threshold=50, corners_threshold=0.01, room_closing_max_length=130, gap_in_wall_min_threshold=5000)\n",
      "        I have copied and changed this function some...\n",
      "        \n",
      "        origin from\n",
      "        https://stackoverflow.com/questions/54274610/crop-each-of-them-using-opencv-python\n",
      "        \n",
      "        @param img: grey scale image of rooms, already eroded and doors removed etc.\n",
      "        @param noise_removal_threshold: Minimal area of blobs to be kept.\n",
      "        @param corners_threshold: Threshold to allow corners. Higher removes more of the house.\n",
      "        @param room_closing_max_length: Maximum line length to add to close off open doors.\n",
      "        @param gap_in_wall_threshold: Minimum number of pixels to identify component as room instead of hole in the wall.\n",
      "        @return: rooms: list of numpy arrays containing boolean masks for each detected room\n",
      "                 colored_house: A colored version of the input image, where each room has a random color.\n",
      "    \n",
      "    mark_outside_black(img, mask)\n",
      "        Mark white background as black\n",
      "        @Param @mandatory img image input\n",
      "        @Param @mandatory mask mask to use\n",
      "        @Return image, mask\n",
      "    \n",
      "    rectContains(rect, pt)\n",
      "        Count if Rect contains point\n",
      "        @Param rect rectangle\n",
      "        @Param pt point\n",
      "        @Return boolean\n",
      "        @source: https://stackoverflow.com/questions/33065834/how-to-detect-if-a-point-is-contained-within-a-bounding-rect-opecv-python\n",
      "    \n",
      "    remove_noise(img, noise_removal_threshold)\n",
      "        Remove noise from image and return mask\n",
      "        Help function for finding room\n",
      "        @Param img @mandatory image to remove noise from\n",
      "        @Param noise_removal_threshold @mandatory threshold for noise\n",
      "        @Return return new mask of image\n",
      "    \n",
      "    wall_filter(gray)\n",
      "        Filter walls\n",
      "        Filter out walls from a grayscale image\n",
      "        @Param image\n",
      "        @Return image of walls\n",
      "\n",
      "FILE\n",
      "    c:\\users\\mingy\\desktop\\新增資料夾\\utils\\floorplantoblenderlib\\detect.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(detect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CubiCasa Pre-Trained Object/Room Segmentation          \n",
    "# (Deep Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\") # Adds higher directory to python modules path.\n",
    "\n",
    "# Import library\n",
    "from utils.FloorplanToBlenderLib import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import cv2 \n",
    "from torch.utils.data import DataLoader\n",
    "from model import get_model\n",
    "from utils.loaders import FloorplanSVG, DictToTensor, Compose, RotateNTurns\n",
    "from utils.plotting import segmentation_plot, polygons_to_image, draw_junction_from_dict, discrete_cmap\n",
    "discrete_cmap()\n",
    "from utils.post_prosessing import split_prediction, get_polygons, split_validation\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "torch.cuda.is_available()\n",
    "\n",
    "rot = RotateNTurns()\n",
    "room_classes = [\"Background\", \"Outdoor\", \"Wall\", \"Kitchen\", \"Living Room\" ,\"Bed Room\", \"Bath\",\n",
    "                \"Entry\", \"Railing\", \"Storage\", \"Garage\", \"Undefined\"]\n",
    "icon_classes = [\"No Icon\", \"Window\", \"Door\", \"Closet\", \"Electrical Applience\" ,\"Toilet\", \"Sink\",\n",
    "                \"Sauna Bench\", \"Fire Place\", \"Bathtub\", \"Chimney\"]\n",
    "\n",
    "model = get_model('hg_furukawa_original', 51)\n",
    "n_classes = 44\n",
    "split = [21, 12, 11]\n",
    "model.conv4_ = torch.nn.Conv2d(256, n_classes, bias=True, kernel_size=1)\n",
    "model.upsample = torch.nn.ConvTranspose2d(n_classes, n_classes, kernel_size=4, stride=4)\n",
    "checkpoint = torch.load('model_best_val_loss_var.pkl',map_location='cpu')\n",
    "\n",
    "model.load_state_dict(checkpoint['model_state'])\n",
    "model.eval()\n",
    "model.cuda()\n",
    "\n",
    "# Create tensor for pytorch\n",
    "img = cv2.imread(img_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # correct color channels\n",
    "\n",
    "# Image transformation to range (-1,1)\n",
    "img = 2 * (img / 255.0) - 1\n",
    "\n",
    "# Move from (h,w,3)--->(3,h,w) as model input dimension is defined like this\n",
    "img = np.moveaxis(img, -1, 0)\n",
    "\n",
    "# Convert to pytorch, enable cuda\n",
    "img = torch.tensor([img.astype(np.float32)]).cuda()\n",
    "n_rooms = 12\n",
    "n_icons = 11\n",
    "\n",
    "with torch.no_grad():\n",
    "    #Check if shape of image is odd or even\n",
    "    size_check = np.array([img.shape[2],img.shape[3]])%2\n",
    "\n",
    "    height = img.shape[2] - size_check[0]\n",
    "    width = img.shape[3] - size_check[1]\n",
    "\n",
    "    img_size = (height, width)\n",
    "\n",
    "    rotations = [(0, 0), (1, -1), (2, 2), (-1, 1)]\n",
    "    pred_count = len(rotations)\n",
    "    prediction = torch.zeros([pred_count, n_classes, height, width])\n",
    "    for i, r in enumerate(rotations):\n",
    "        forward, back = r\n",
    "        # We rotate first the image\n",
    "        rot_image = rot(img, 'tensor', forward)\n",
    "        pred = model(rot_image)\n",
    "        # We rotate prediction back\n",
    "        pred = rot(pred, 'tensor', back)\n",
    "        # We fix heatmaps\n",
    "        pred = rot(pred, 'points', back)\n",
    "        # We make sure the size is correct\n",
    "        pred = F.interpolate(pred, size=(height, width), mode='bilinear', align_corners=True)\n",
    "        # We add the prediction to output\n",
    "        prediction[i] = pred[0]\n",
    "\n",
    "prediction = torch.mean(prediction, 0, True)\n",
    "\n",
    "\n",
    "rooms_pred = F.softmax(prediction[0, 21:21+12], 0).cpu().data.numpy()\n",
    "rooms_pred = np.argmax(rooms_pred, axis=0)\n",
    "\n",
    "icons_pred = F.softmax(prediction[0, 21+12:], 0).cpu().data.numpy()\n",
    "icons_pred = np.argmax(icons_pred, axis=0)\n",
    "\n",
    "heatmaps, rooms, icons = split_prediction(prediction, img_size, split)\n",
    "polygons, types, room_polygons, room_types = get_polygons((heatmaps, rooms, icons), 0.2, [1, 2])\n",
    "\n",
    "wall_polygon_numbers=[i for i,j in enumerate(types) if j['type']=='wall']\n",
    "boxes=[]\n",
    "for i,j in enumerate(polygons):\n",
    "    if i in wall_polygon_numbers:\n",
    "        temp=[]\n",
    "        for k in j:\n",
    "            temp.append(np.array([k]))\n",
    "        boxes.append(np.array(temp))\n",
    "        \n",
    "verts, faces, wall_amount = transform.create_nx4_verts_and_faces(boxes, wall_height, scale)\n",
    "\n",
    "# Create top walls verts\n",
    "verts = []\n",
    "for box in boxes:\n",
    "    verts.extend([transform.scale_point_to_vector(box, scale, 0)])\n",
    "\n",
    "# create faces\n",
    "faces = []\n",
    "for room in verts:\n",
    "    count = 0\n",
    "    temp = ()\n",
    "    for _ in room:\n",
    "        temp = temp + (count,)\n",
    "        count += 1\n",
    "    faces.append([(temp)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pol_room_seg, pol_icon_seg = polygons_to_image(polygons, types, room_polygons, room_types, height, width)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.axis('off')\n",
    "rseg = ax.imshow(pol_room_seg, cmap='rooms', vmin=0, vmax=n_rooms-0.1)\n",
    "cbar = plt.colorbar(rseg, ticks=np.arange(n_rooms) + 0.5, fraction=0.046, pad=0.01)\n",
    "cbar.ax.set_yticklabels(room_classes, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(12,12))\n",
    "ax = plt.subplot(1, 1, 1)\n",
    "ax.axis('off')\n",
    "iseg = ax.imshow(pol_icon_seg, cmap='icons', vmin=0, vmax=n_icons-0.1)\n",
    "cbar = plt.colorbar(iseg, ticks=np.arange(n_icons) + 0.5, fraction=0.046, pad=0.01)\n",
    "cbar.ax.set_yticklabels(icon_classes, fontsize=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate data files and Blender files (Using CubiCasa and Super-Resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import FloorplanToSTL as stl\n",
    "import config\n",
    "\n",
    "# can specify or use default paths in config files\n",
    "stl.createFloorPlan(image_path = config.image_path, target_path = config.target_path, SR_Check=True)\n",
    "\n",
    "# Note: USE SR_Check = False for Original Image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.2+cu111\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
